{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:29:26.772305Z",
     "start_time": "2025-08-03T12:29:26.344907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Generate a small synthetic fraud detection dataset for demonstration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 1000 samples\n",
    "n_samples = 1000\n",
    "# Features: amount, transaction_time, is_foreign, is_high_risk_country\n",
    "amount = np.random.exponential(scale=100, size=n_samples)\n",
    "transaction_time = np.random.randint(0, 24, size=n_samples)  # hour of day\n",
    "is_foreign = np.random.binomial(1, 0.1, size=n_samples)  # 10% foreign\n",
    "is_high_risk_country = np.random.binomial(1, 0.05, size=n_samples)  # 5% high risk\n",
    "\n",
    "# Generate labels: fraud is more likely for high amount, foreign, high risk country, odd hours\n",
    "fraud = (\n",
    "        (amount > 200).astype(int) +\n",
    "        (is_foreign == 1).astype(int) +\n",
    "        (is_high_risk_country == 1).astype(int) +\n",
    "        ((transaction_time < 6) | (transaction_time > 22)).astype(int)\n",
    ")\n",
    "# If sum of risk factors >= 2, label as fraud\n",
    "labels = (fraud >= 2).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "fraud_df = pd.DataFrame({\n",
    "    'amount': amount,\n",
    "    'transaction_time': transaction_time,\n",
    "    'is_foreign': is_foreign,\n",
    "    'is_high_risk_country': is_high_risk_country,\n",
    "    'fraud': labels\n",
    "})\n",
    "\n",
    "print('Sample of synthetic fraud detection data:')\n",
    "display(fraud_df.head())\n",
    "print('Fraud distribution:')\n",
    "print(fraud_df[\"fraud\"].value_counts())\n",
    "\n",
    "# Save the generated synthetic fraud detection DataFrame to a CSV file for later use.\n",
    "fraud_df.to_csv('synthetic_fraud_data.csv', index=False)\n",
    "print('Data saved to synthetic_fraud_data.csv')\n"
   ],
   "id": "b47955317fa1038e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of synthetic fraud detection data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       amount  transaction_time  is_foreign  is_high_risk_country  fraud\n",
       "0   46.926809                14           0                     0      0\n",
       "1  301.012143                11           1                     0      1\n",
       "2  131.674569                15           0                     0      0\n",
       "3   91.294255                23           1                     0      1\n",
       "4   16.962487                18           0                     0      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>is_foreign</th>\n",
       "      <th>is_high_risk_country</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.926809</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301.012143</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.674569</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.294255</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.962487</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud distribution:\n",
      "fraud\n",
      "0    897\n",
      "1    103\n",
      "Name: count, dtype: int64\n",
      "Data saved to synthetic_fraud_data.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Read the synthetic fraud detection data from file\n",
    "import pandas as pd\n",
    "fraud_df = pd.read_csv('synthetic_fraud_data.csv')\n",
    "print('Loaded data sample:')\n",
    "display(fraud_df.head())\n",
    "print('Fraud distribution:')\n",
    "print(fraud_df['fraud'].value_counts())\n"
   ],
   "id": "7f8302b3157b2125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Check for missing values\n",
    "print('Missing values in each column:')\n",
    "print(fraud_df.isnull().sum())\n",
    "\n",
    "# 2. Summary statistics\n",
    "print('\\nSummary statistics:')\n",
    "print(fraud_df.describe())\n",
    "\n",
    "# 3. Visualize feature distributions\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].hist(fraud_df['amount'], bins=30, color='skyblue')\n",
    "axs[0, 0].set_title('Amount Distribution')\n",
    "axs[0, 1].hist(fraud_df['transaction_time'], bins=24, color='orange')\n",
    "axs[0, 1].set_title('Transaction Time Distribution')\n",
    "axs[1, 0].bar(['No', 'Yes'], fraud_df['is_foreign'].value_counts().sort_index(), color='green')\n",
    "axs[1, 0].set_title('Is Foreign')\n",
    "axs[1, 1].bar(['No', 'Yes'], fraud_df['is_high_risk_country'].value_counts().sort_index(), color='red')\n",
    "axs[1, 1].set_title('Is High Risk Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Check class balance\n",
    "print('\\nFraud class balance:')\n",
    "print(fraud_df['fraud'].value_counts(normalize=True))\n"
   ],
   "id": "663031787c465e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4: Data Preprocessing (train-test split and scaling)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Split features and target\n",
    "y = fraud_df['fraud']\n",
    "X = fraud_df.drop('fraud', axis=1)\n",
    "\n",
    "# 2. Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['amount', 'transaction_time']] = scaler.fit_transform(X_train[['amount', 'transaction_time']])\n",
    "X_test[['amount', 'transaction_time']] = scaler.transform(X_test[['amount', 'transaction_time']])\n",
    "\n",
    "# 4. Show shapes\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}, {y_test.shape}')\n",
    "print('Sample of scaled training data:')\n",
    "display(X_train.head())\n",
    "\n",
    "# Save train and test data for later use\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "print('Train and test data saved as X_train.csv, X_test.csv, y_train.csv, y_test.csv')\n"
   ],
   "id": "6cf8d5b3fdd14403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:56:41.527417Z",
     "start_time": "2025-08-03T12:56:41.504662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Train and evaluate a Logistic Regression model for fraud detection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load train and test data (in case running this cell independently)\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Train logistic regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f'ROC AUC Score: {roc_auc_score(y_test, y_proba):.3f}')\n"
   ],
   "id": "a066557d1b7a7e0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[178   1]\n",
      " [ 12   9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.937     0.994     0.965       179\n",
      "           1      0.900     0.429     0.581        21\n",
      "\n",
      "    accuracy                          0.935       200\n",
      "   macro avg      0.918     0.711     0.773       200\n",
      "weighted avg      0.933     0.935     0.924       200\n",
      "\n",
      "ROC AUC Score: 0.930\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 6: Train and evaluate a Neural Network for fraud detection\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load train and test data (in case running this cell independently)\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Build a simple neural network model\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1, validation_split=0.1)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "y_proba_nn = nn_model.predict(X_test).ravel()\n",
    "\n",
    "# Evaluation\n",
    "print('Confusion Matrix (Neural Network):')\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print('\\nClassification Report (Neural Network):')\n",
    "print(classification_report(y_test, y_pred_nn, digits=3))\n",
    "print(f'ROC AUC Score (Neural Network): {roc_auc_score(y_test, y_proba_nn):.3f}')\n"
   ],
   "id": "b694c1995560b44d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T13:13:20.747979Z",
     "start_time": "2025-08-03T13:13:15.414708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 7: Improve models and compare Logistic Regression and Neural Network\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load train and test data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# 1. Use SMOTE to oversample the minority class in training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# 2. Compute class weights for use in models\n",
    "classes = np.unique(y_train)\n",
    "class_weights = dict(zip(classes, compute_class_weight('balanced', classes=classes, y=y_train)))\n",
    "\n",
    "# 3. Improved Logistic Regression (with class weights)\n",
    "logreg = LogisticRegression(random_state=42, class_weight=class_weights, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_lr = logreg.predict(X_test)\n",
    "y_proba_lr = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 4. Improved Neural Network (with class weights and SMOTE data)\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train_sm, y_train_sm, epochs=50, batch_size=32, verbose=0, validation_split=0.1, class_weight=class_weights)\n",
    "y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "y_proba_nn = nn_model.predict(X_test).ravel()\n",
    "\n",
    "# 5. Compare results\n",
    "print('--- Improved Logistic Regression ---')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr, digits=3))\n",
    "print(f'ROC AUC Score: {roc_auc_score(y_test, y_proba_lr):.3f}\\n')\n",
    "\n",
    "print('--- Improved Neural Network ---')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(classification_report(y_test, y_pred_nn, digits=3))\n",
    "print(f'ROC AUC Score: {roc_auc_score(y_test, y_proba_nn):.3f}')\n"
   ],
   "id": "5d9327ed85adfcb5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sefa.balik/projects/datascience/neuralnet/.venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/Users/sefa.balik/projects/datascience/neuralnet/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step \n",
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\n",
      "--- Improved Logistic Regression ---\n",
      "Confusion Matrix:\n",
      "[[157  22]\n",
      " [  2  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.877     0.929       179\n",
      "           1      0.463     0.905     0.613        21\n",
      "\n",
      "    accuracy                          0.880       200\n",
      "   macro avg      0.725     0.891     0.771       200\n",
      "weighted avg      0.932     0.880     0.896       200\n",
      "\n",
      "ROC AUC Score: 0.950\n",
      "\n",
      "--- Improved Neural Network ---\n",
      "Confusion Matrix:\n",
      "[[153  26]\n",
      " [  0  21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.855     0.922       179\n",
      "           1      0.447     1.000     0.618        21\n",
      "\n",
      "    accuracy                          0.870       200\n",
      "   macro avg      0.723     0.927     0.770       200\n",
      "weighted avg      0.942     0.870     0.890       200\n",
      "\n",
      "ROC AUC Score: 0.997\n"
     ]
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
