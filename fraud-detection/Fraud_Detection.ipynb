{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:29:26.772305Z",
     "start_time": "2025-08-03T12:29:26.344907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Generate a small synthetic fraud detection dataset for demonstration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 1000 samples\n",
    "n_samples = 1000\n",
    "# Features: amount, transaction_time, is_foreign, is_high_risk_country\n",
    "amount = np.random.exponential(scale=100, size=n_samples)\n",
    "transaction_time = np.random.randint(0, 24, size=n_samples)  # hour of day\n",
    "is_foreign = np.random.binomial(1, 0.1, size=n_samples)  # 10% foreign\n",
    "is_high_risk_country = np.random.binomial(1, 0.05, size=n_samples)  # 5% high risk\n",
    "\n",
    "# Generate labels: fraud is more likely for high amount, foreign, high risk country, odd hours\n",
    "fraud = (\n",
    "        (amount > 200).astype(int) +\n",
    "        (is_foreign == 1).astype(int) +\n",
    "        (is_high_risk_country == 1).astype(int) +\n",
    "        ((transaction_time < 6) | (transaction_time > 22)).astype(int)\n",
    ")\n",
    "# If sum of risk factors >= 2, label as fraud\n",
    "labels = (fraud >= 2).astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "fraud_df = pd.DataFrame({\n",
    "    'amount': amount,\n",
    "    'transaction_time': transaction_time,\n",
    "    'is_foreign': is_foreign,\n",
    "    'is_high_risk_country': is_high_risk_country,\n",
    "    'fraud': labels\n",
    "})\n",
    "\n",
    "print('Sample of synthetic fraud detection data:')\n",
    "display(fraud_df.head())\n",
    "print('Fraud distribution:')\n",
    "print(fraud_df[\"fraud\"].value_counts())\n",
    "\n",
    "# Save the generated synthetic fraud detection DataFrame to a CSV file for later use.\n",
    "fraud_df.to_csv('synthetic_fraud_data.csv', index=False)\n",
    "print('Data saved to synthetic_fraud_data.csv')\n"
   ],
   "id": "b47955317fa1038e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of synthetic fraud detection data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       amount  transaction_time  is_foreign  is_high_risk_country  fraud\n",
       "0   46.926809                14           0                     0      0\n",
       "1  301.012143                11           1                     0      1\n",
       "2  131.674569                15           0                     0      0\n",
       "3   91.294255                23           1                     0      1\n",
       "4   16.962487                18           0                     0      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>is_foreign</th>\n",
       "      <th>is_high_risk_country</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.926809</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301.012143</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.674569</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>91.294255</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.962487</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud distribution:\n",
      "fraud\n",
      "0    897\n",
      "1    103\n",
      "Name: count, dtype: int64\n",
      "Data saved to synthetic_fraud_data.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Read the synthetic fraud detection data from file\n",
    "import pandas as pd\n",
    "fraud_df = pd.read_csv('synthetic_fraud_data.csv')\n",
    "print('Loaded data sample:')\n",
    "display(fraud_df.head())\n",
    "print('Fraud distribution:')\n",
    "print(fraud_df['fraud'].value_counts())\n"
   ],
   "id": "7f8302b3157b2125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Check for missing values\n",
    "print('Missing values in each column:')\n",
    "print(fraud_df.isnull().sum())\n",
    "\n",
    "# 2. Summary statistics\n",
    "print('\\nSummary statistics:')\n",
    "print(fraud_df.describe())\n",
    "\n",
    "# 3. Visualize feature distributions\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].hist(fraud_df['amount'], bins=30, color='skyblue')\n",
    "axs[0, 0].set_title('Amount Distribution')\n",
    "axs[0, 1].hist(fraud_df['transaction_time'], bins=24, color='orange')\n",
    "axs[0, 1].set_title('Transaction Time Distribution')\n",
    "axs[1, 0].bar(['No', 'Yes'], fraud_df['is_foreign'].value_counts().sort_index(), color='green')\n",
    "axs[1, 0].set_title('Is Foreign')\n",
    "axs[1, 1].bar(['No', 'Yes'], fraud_df['is_high_risk_country'].value_counts().sort_index(), color='red')\n",
    "axs[1, 1].set_title('Is High Risk Country')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Check class balance\n",
    "print('\\nFraud class balance:')\n",
    "print(fraud_df['fraud'].value_counts(normalize=True))\n"
   ],
   "id": "663031787c465e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4: Data Preprocessing (train-test split and scaling)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Split features and target\n",
    "y = fraud_df['fraud']\n",
    "X = fraud_df.drop('fraud', axis=1)\n",
    "\n",
    "# 2. Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train[['amount', 'transaction_time']] = scaler.fit_transform(X_train[['amount', 'transaction_time']])\n",
    "X_test[['amount', 'transaction_time']] = scaler.transform(X_test[['amount', 'transaction_time']])\n",
    "\n",
    "# 4. Show shapes\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}, {y_test.shape}')\n",
    "print('Sample of scaled training data:')\n",
    "display(X_train.head())\n",
    "\n",
    "# Save train and test data for later use\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "print('Train and test data saved as X_train.csv, X_test.csv, y_train.csv, y_test.csv')\n"
   ],
   "id": "6cf8d5b3fdd14403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:56:41.527417Z",
     "start_time": "2025-08-03T12:56:41.504662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Train and evaluate a Logistic Regression model for fraud detection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load train and test data (in case running this cell independently)\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Train logistic regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "print(f'ROC AUC Score: {roc_auc_score(y_test, y_proba):.3f}')\n"
   ],
   "id": "a066557d1b7a7e0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[178   1]\n",
      " [ 12   9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.937     0.994     0.965       179\n",
      "           1      0.900     0.429     0.581        21\n",
      "\n",
      "    accuracy                          0.935       200\n",
      "   macro avg      0.918     0.711     0.773       200\n",
      "weighted avg      0.933     0.935     0.924       200\n",
      "\n",
      "ROC AUC Score: 0.930\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-03T12:53:40.855225Z",
     "start_time": "2025-08-03T12:53:34.561941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Train and evaluate a Neural Network for fraud detection\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load train and test data (in case running this cell independently)\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Build a simple neural network model\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1, validation_split=0.1)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_nn = (nn_model.predict(X_test) > 0.5).astype(int).ravel()\n",
    "y_proba_nn = nn_model.predict(X_test).ravel()\n",
    "\n",
    "# Evaluation\n",
    "print('Confusion Matrix (Neural Network):')\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print('\\nClassification Report (Neural Network):')\n",
    "print(classification_report(y_test, y_pred_nn, digits=3))\n",
    "print(f'ROC AUC Score (Neural Network): {roc_auc_score(y_test, y_proba_nn):.3f}')\n"
   ],
   "id": "e916a779dc35c860",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sefa.balik/projects/datascience/neuralnet/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sefa.balik/projects/datascience/neuralnet/.venv/lib/python3.9/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.6929 - loss: 0.6818 - val_accuracy: 0.8750 - val_loss: 0.6300\n",
      "Epoch 2/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8943 - loss: 0.5955 - val_accuracy: 0.8750 - val_loss: 0.5656\n",
      "Epoch 3/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8917 - loss: 0.5112 - val_accuracy: 0.8750 - val_loss: 0.5119\n",
      "Epoch 4/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8913 - loss: 0.4600 - val_accuracy: 0.8750 - val_loss: 0.4627\n",
      "Epoch 5/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8982 - loss: 0.3976 - val_accuracy: 0.8750 - val_loss: 0.4212\n",
      "Epoch 6/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8970 - loss: 0.3656 - val_accuracy: 0.8750 - val_loss: 0.3871\n",
      "Epoch 7/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8907 - loss: 0.3432 - val_accuracy: 0.8750 - val_loss: 0.3610\n",
      "Epoch 8/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8873 - loss: 0.3243 - val_accuracy: 0.8750 - val_loss: 0.3422\n",
      "Epoch 9/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8979 - loss: 0.2828 - val_accuracy: 0.8750 - val_loss: 0.3265\n",
      "Epoch 10/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9071 - loss: 0.2652 - val_accuracy: 0.8750 - val_loss: 0.3137\n",
      "Epoch 11/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9172 - loss: 0.2521 - val_accuracy: 0.8750 - val_loss: 0.3041\n",
      "Epoch 12/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8912 - loss: 0.2713 - val_accuracy: 0.8875 - val_loss: 0.2955\n",
      "Epoch 13/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.8986 - loss: 0.2414 - val_accuracy: 0.9125 - val_loss: 0.2875\n",
      "Epoch 14/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9078 - loss: 0.2509 - val_accuracy: 0.9125 - val_loss: 0.2791\n",
      "Epoch 15/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9085 - loss: 0.2129 - val_accuracy: 0.9250 - val_loss: 0.2727\n",
      "Epoch 16/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9171 - loss: 0.2241 - val_accuracy: 0.9250 - val_loss: 0.2638\n",
      "Epoch 17/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9324 - loss: 0.1958 - val_accuracy: 0.9250 - val_loss: 0.2539\n",
      "Epoch 18/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9229 - loss: 0.2022 - val_accuracy: 0.9375 - val_loss: 0.2438\n",
      "Epoch 19/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9237 - loss: 0.2098 - val_accuracy: 0.9375 - val_loss: 0.2339\n",
      "Epoch 20/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9311 - loss: 0.1727 - val_accuracy: 0.9375 - val_loss: 0.2264\n",
      "Epoch 21/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9375 - loss: 0.1691 - val_accuracy: 0.9375 - val_loss: 0.2172\n",
      "Epoch 22/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9264 - loss: 0.1997 - val_accuracy: 0.9375 - val_loss: 0.2112\n",
      "Epoch 23/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9411 - loss: 0.1694 - val_accuracy: 0.9375 - val_loss: 0.2008\n",
      "Epoch 24/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9359 - loss: 0.1786 - val_accuracy: 0.9375 - val_loss: 0.1944\n",
      "Epoch 25/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9314 - loss: 0.1731 - val_accuracy: 0.9375 - val_loss: 0.1858\n",
      "Epoch 26/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9448 - loss: 0.1450 - val_accuracy: 0.9375 - val_loss: 0.1793\n",
      "Epoch 27/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9468 - loss: 0.1286 - val_accuracy: 0.9375 - val_loss: 0.1714\n",
      "Epoch 28/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9464 - loss: 0.1270 - val_accuracy: 0.9500 - val_loss: 0.1640\n",
      "Epoch 29/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9365 - loss: 0.1593 - val_accuracy: 0.9625 - val_loss: 0.1596\n",
      "Epoch 30/30\n",
      "\u001B[1m23/23\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.9520 - loss: 0.1279 - val_accuracy: 0.9750 - val_loss: 0.1508\n",
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step \n",
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step\n",
      "Confusion Matrix (Neural Network):\n",
      "[[179   0]\n",
      " [ 11  10]]\n",
      "\n",
      "Classification Report (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.942     1.000     0.970       179\n",
      "           1      1.000     0.476     0.645        21\n",
      "\n",
      "    accuracy                          0.945       200\n",
      "   macro avg      0.971     0.738     0.808       200\n",
      "weighted avg      0.948     0.945     0.936       200\n",
      "\n",
      "ROC AUC Score (Neural Network): 0.928\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
